# RAG Chatbot â€“ Intelligent Conversational AI with Custom Knowledge

An **Intelligent Chatbot** powered by **Retrieval-Augmented Generation (RAG)** that combines the reasoning capabilities of **Large Language Models (LLMs)** with **domain-specific knowledge retrieval**.  
This project demonstrates a **hybrid chatbot system** that delivers **accurate, context-aware, and grounded responses** using **custom documents** as its knowledge base.

---

## ðŸ“Œ Features
- **Domain-Specific Knowledge** â€“ Preloaded with documents related to:
  - Web Development  
  - Software Engineering  
  - QA Engineering  
  - AI/ML Engineering  
  - Cybersecurity Analysis  
  - And more computing-related fields
- **RAG Pipeline** â€“ Combines vector search with LLM reasoning.
- **Multi-Document Support** â€“ Ingest and search across multiple files.
- **Streamlit Frontend** â€“ Simple, interactive UI for real-time queries.
- **Expandable** â€“ Easily update or add new knowledge sources.

---

## ðŸ› ï¸ Tech Stack
- **Backend**: Python, LangChain, FAISS / ChromaDB
- **Frontend**: Streamlit
- **LLM**: OpenAI GPT / other supported LLM APIs
- **Document Handling**: PDF/Text ingestion, Embedding generation

---

## ðŸ“‚ Project Structure
```
RAG_CHATBOT/
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py              # Streamlit UI
â”‚
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ rag_pipeline.py     # RAG pipeline logic
â”‚
â”œâ”€â”€ data/                   # Knowledge base documents
â”‚
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md               # Project documentation
```

---

## ðŸš€ How It Works
1. **Document Loading** â€“ PDFs or text files are loaded from the `data/` folder.  
2. **Embedding Generation** â€“ Text is converted into numerical embeddings using an embedding model.  
3. **Vector Storage** â€“ Embeddings are stored in a vector database (FAISS/Chroma).  
4. **Query Processing** â€“ Userâ€™s query is embedded and matched against stored vectors.  
5. **LLM Response Generation** â€“ Retrieved context is passed to the LLM to produce an accurate, grounded answer.

---

## ðŸ“¦ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/RAG_CHATBOT.git
cd RAG_CHATBOT
```

### 2ï¸âƒ£ Create a Virtual Environment
```bash
python -m venv .venv
.venv\Scripts\activate   # Windows
# source .venv/bin/activate  # macOS/Linux
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add Your API Key
Create a `.env` file in the root folder:
```
OPENAI_API_KEY=your_api_key_here
```

### 5ï¸âƒ£ Run the App
```bash
streamlit run frontend/app.py
```

---

## ðŸ§  Example Use Case
> **Example Query:** *"What are the skills required for an AI/ML Engineer?"*  
The chatbot retrieves relevant data from the stored documents, enriches it with LLM reasoning, and provides an accurate, well-structured answer.

---

## ðŸ“ˆ Future Improvements
- Support for additional file formats (Word, CSV, HTML)
- Real-time web scraping for latest updates
- Enhanced UI with chat history and file upload
- Multiple LLM backend support

---

## ðŸ“œ License
This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.
